{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccdcd2c8",
   "metadata": {},
   "source": [
    "# Object Detection and Classification of Screws\n",
    "The goal of this project is to use a dataset of photographs to train a model to detect and identify\n",
    "an assorment of screws and nuts. The dataset was taken from the MVTec Screws Dataset. Object\n",
    "detection is a challenging problem in machinelearning and continues to be a very active area of\n",
    "research with new algorithms and processes being developed regularly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d35ceac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-04 22:35:18.114887: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-04 22:35:18.318561: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from glob import glob, iglob\n",
    "import os\n",
    "import pathlib\n",
    "import pprint as pp\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "from numpy.typing import ArrayLike, NDArray\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle, Polygon, Circle\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "from matplotlib.collections import PolyCollection\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, layers, losses, metrics\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import image_dataset_from_directory, load_img\n",
    "\n",
    "\n",
    "from src.utils.funcs import init_COCO, process_img_annotations, rotate\n",
    "from src.utils.classes import CategoricalDataGen, bbox_worker\n",
    "from src.models.models import IoU, YOLOLoss\n",
    "from src.utils.box_cutter import BoundingBox_Processor\n",
    "\n",
    "from IPython.display import HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a3d70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pprint.PrettyPrinter at 0x7f57d2b87a90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be0936c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f03cbd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "%aimport src.utils.box_cutter\n",
    "%aimport src.utils.funcs\n",
    "%aimport src.utils.classes\n",
    "%aimport src.models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e862af",
   "metadata": {},
   "source": [
    "### Data Cleaning/Preparation\n",
    "The data presented several challenges when preparing it for training. The bounding-box format was\n",
    "similar to the COCO format but did not seem to match what most plug and play tool chains expected.\n",
    "This meant reworking the data multiple times and in a multitude of ways. \n",
    "\n",
    "Data manipulation led to the writing of several custom tools to handle various aspects of preparing\n",
    "the data. These included:\n",
    "* Tools to create coordinate points for bounding box corners from the row, col, width, height, angle format in the data.\n",
    "* Tools to create compatible label formats for training.\n",
    "* Tools to calculate IoU and GIoU for unaligned bounding boxes.\n",
    "* Tools to provide necessary calculations from the label data to the loss function of a YOLO-like model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b820d4ef",
   "metadata": {},
   "source": [
    "## Initializing Data with Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0e4dd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Found 269 train images\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Found 55 val images\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Found 60 test images\n"
     ]
    }
   ],
   "source": [
    "data = init_COCO(\"./data/\", [\"train\", \"val\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "928a170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = CategoricalDataGen('train', data, \"./data/images\", target_size=(384, 512))\n",
    "val = CategoricalDataGen('val', data, \"./data/images\", target_size=(384, 512))\n",
    "test = CategoricalDataGen('test', data, \"./data/images\", target_size=(384, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b44fa89f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 192 and the array at index 1 has size 384",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m X_val, y_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m192\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m3\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), np\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m13\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m train\u001b[38;5;241m.\u001b[39mbatch(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(y_train, y, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m val\u001b[38;5;241m.\u001b[39mbatch(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m):\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/conda/envs/tf_env/lib/python3.10/site-packages/numpy/lib/function_base.py:5444\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     values \u001b[38;5;241m=\u001b[39m ravel(values)\n\u001b[1;32m   5443\u001b[0m     axis \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 5444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 192 and the array at index 1 has size 384"
     ]
    }
   ],
   "source": [
    "X_train, y_train = np.empty((0, 192, 256, 3), dtype=np.float32), np.empty((0, 13), dtype=np.float32)\n",
    "X_val, y_val = np.empty((0, 192, 256, 3), dtype=np.float32), np.empty((0, 13), dtype=np.float32)\n",
    "\n",
    "for x, y in train.batch(batch_size=16):\n",
    "    X_train = np.append(X_train, x, axis=0)\n",
    "    y_train = np.append(y_train, y, axis=0)\n",
    "\n",
    "for x, y in val.batch(batch_size=16):\n",
    "    X_val = np.append(X_val, x, axis=0)\n",
    "    y_val = np.append(y_val, y, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2d5c93",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "The Dataset for this project consists of several hundred images of around 4500 thousand pieces of\n",
    "construction hardware belonging to 13 different classes. The mostly represent types of common screws\n",
    "however the images also contain nuts of varying sizes. Each size and type of hardware belongs to\n",
    "its own class.\n",
    "\n",
    "![EDA](./images/screws_with_label_boxes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f9001",
   "metadata": {},
   "source": [
    "## Splitting Data into Single Class Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5402d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val.crop_dataset(\"/content/data\")\n",
    "training.crop_dataset(\"/content/data\")\n",
    "testing.crop_dataset(\"/content/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d133fa45",
   "metadata": {},
   "source": [
    "![cropped classes](./images/cropped_classes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d829c9",
   "metadata": {},
   "source": [
    "# Model 1\n",
    "This model did well at classifying the dataset, scoring around a %97 percent accuracy after only a few epochs. It consisted of a few convolutional blocks that feed two fully-connected dense layers that output the class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44ac18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_classifier(input_shape, num_classes):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv2D(16, 3, strides=2, activation='relu', padding='same')(inputs)\n",
    "    x = layers.Conv2D(32, 3, strides=2, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(64, 3, strides=2, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(128, 3, strides=1, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(128, 3, strides=2, activation='relu', padding='same')(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(13, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c7f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam',\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aca054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = classifier.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b134fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10.5, 5))\n",
    "\n",
    "ax[0].plot(history.history['loss'], label=\"Training\")\n",
    "ax[0].plot(history.history['val_loss'], label=\"Validation\")\n",
    "ax[0].set_title('Sparse Categorical Crossentropy')\n",
    "ax[0].set_xlabel('epochs')\n",
    "ax[0].set_ylabel('loss')\n",
    "\n",
    "ax[1].plot(history.history['accuracy'], label=\"Training\")\n",
    "ax[1].plot(history.history['val_accuracy'], label=\"Validation\")\n",
    "ax[1].set_title('Accuracy')\n",
    "ax[1].set_xlabel('epochs')\n",
    "ax[1].set_ylabel('accuracy')\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "fig.suptitle(\"10 Epoch Simple Classifier\", fontsize=18)\n",
    "\n",
    "plt.savefig(\"/content/gdrive/MyDrive/colab_output/images/simple_model_10epoch.png\")\n",
    "plt.show("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfac876",
   "metadata": {},
   "source": [
    "\n",
    "![Simple Model Metrics](./images/simple_model_10epoch.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97cd402",
   "metadata": {},
   "source": [
    "## Inspecting Convolution Layer Outputs\n",
    "This method of inspecting individual layer responses to model inputs comes from Francois Chollet's book <i>Deep Learning with Python</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75afbe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = []\n",
    "layer_names = []\n",
    "for layer in classifier.layers:\n",
    "    if isinstance(layer, (layers.Conv2D)):\n",
    "        layer_outputs.append(layer.output)\n",
    "        layer_names.append(layer.name)\n",
    "activation_model = tf.keras.Model(inputs=classifier.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8173c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = activation_model.predict(val_ds.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f6f7d",
   "metadata": {},
   "source": [
    "### First Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381437bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_activation_layer = activations[0]\n",
    "\n",
    "for img in range(first_activation_layer.shape[0]):\n",
    "    fig, axs = plt.subplots(4, 4, figsize=(16, 16))\n",
    "    for i, ax_row in zip(range(4), axs):\n",
    "        for j, ax in zip(range(4), ax_row):\n",
    "            ax.imshow(first_activation_layer[img, :, :, j+i], cmap='magma')\n",
    "            ax.axis('off')\n",
    "            ax.set_title(f\"{img} filter {i+j+1}\")\n",
    "    plt.savefig(f'/content/gdrive/MyDrive/colab_output/images/conv_layer_outputs_{img:03d}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0393ad56",
   "metadata": {},
   "source": [
    "### All the Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c8d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_per_row = 16\n",
    "for i_num in range(32):\n",
    "    for layer_name, layer_activation in zip(layer_names, activations):\n",
    "        n_features = layer_activation.shape[-1]\n",
    "        size = layer_activation.shape[1]\n",
    "        n_cols = n_features // images_per_row\n",
    "        display_grid = np.zeros(((size + 1) * n_cols - 1,\n",
    "                                images_per_row * (size + 1) - 1))\n",
    "        for col in range(n_cols):\n",
    "            for row in range(images_per_row):\n",
    "                channel_index = col * images_per_row + row\n",
    "                channel_image = layer_activation[i_num, :, :, channel_index].copy() \n",
    "\n",
    "                if channel_image.sum() != 0:\n",
    "                    channel_image -= channel_image.mean()\n",
    "                    channel_image /= channel_image.std()\n",
    "                    channel_image *= 64\n",
    "                    channel_image += 128\n",
    "                channel_image = np.clip(channel_image, 0, 255).astype(\"uint8\")\n",
    "                display_grid[col * (size + 1): (col + 1) * size + col,\n",
    "                             row * (size + 1) : (row + 1) * size + row] = channel_image\n",
    "        scale = 1. / size\n",
    "        plt.figure(figsize=(scale * display_grid.shape[1], scale * display_grid.shape[0]))\n",
    "\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(display_grid, aspect=\"auto\", cmap=\"magma\")\n",
    "        plt.savefig(f\"/content/gdrive/MyDrive/colab_output/images/conv_layers/{i_num}_conv_visualization_{layer_name}.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d267150c",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b0824e",
   "metadata": {},
   "source": [
    "## Model Layers Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17a942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_block(x, filters, kernel_size=3, reps:int=2, pooling:bool=False, **kwargs):\n",
    "  residual = x\n",
    "  options = {}\n",
    "  if kwargs:\n",
    "    options.update(**kwargs)\n",
    "  for rep in range(reps):\n",
    "    if not rep:\n",
    "      options.update({'strides': 2})\n",
    "    else:\n",
    "      options['strides'] = 1\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(filters, kernel_size, padding=\"same\", use_bias=False, **options)(x)\n",
    "  \n",
    "  if pooling:\n",
    "    x = layers.MaxPooling2D(kernel_size, strides=2, padding=\"same\")(x)\n",
    "    # residual = layers.Conv2D(filters, 1, strides=2)(residual)\n",
    "  # elif filters != residual.shape[-1]:\n",
    "  #   residual = layers.Conv2D(filters, 1)(residual)\n",
    "  \n",
    "  # x = layers.add([x, residual])\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_T_block(x, filters, kernel_size=3, reps:int=2, **kwargs):\n",
    "  residual = x\n",
    "  options = {'strides': 2}\n",
    "  if kwargs:\n",
    "    options.update(**kwargs)\n",
    "  for rep in range(reps):\n",
    "    if not rep:\n",
    "      options.update({'strides': 2})\n",
    "    else:\n",
    "      options['strides'] = 1\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.Conv2DTranspose(filters, kernel_size, padding=\"same\", use_bias=False, **options)(x)\n",
    "  \n",
    "  # residual = layers.Conv2D(filters, 1)(residual)\n",
    "  \n",
    "  # x = layers.add([x, residual])\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23845a74",
   "metadata": {},
   "source": [
    "# Model 2\n",
    "## Model Defintion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fdd284",
   "metadata": {},
   "source": [
    "My second iteration of the model attempted to use a model structure similar to an Autoencoder. Using\n",
    "the bounding box labels from the dataset to make pixel masks of the classes I then tried several\n",
    "different configurations to get the model to predict the class labeled masks from each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5c0028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape, filter_blocks:List, rescaling:bool=False, **kwargs):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    if rescaling:\n",
    "        x = layers.Rescaling(1./255)(inputs)\n",
    "        x = layers.Conv2D(filter_blocks[0], kernel_size=5, padding='same', use_bias=False)(x)\n",
    "    else:\n",
    "        x = layers.Conv2D(filter_blocks[0], kernel_size=5, padding='same', use_bias=False)(inputs)\n",
    "\n",
    "    for block in filter_blocks:\n",
    "        x = conv2d_block(x, block, **kwargs)\n",
    "\n",
    "        r_filter_blocks = reversed(filter_blocks)\n",
    "    for t_block in r_filter_blocks:\n",
    "        x = conv2d_T_block(x, t_block, **kwargs)\n",
    "\n",
    "    outputs = layers.Conv2D(1, 3, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f263b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Init and Config Values\n",
    "filters = [32, 64, 128, 256, 512]\n",
    "input_shape = x_2_train_final.shape[1:-1] + (1,)\n",
    "print(input_shape)\n",
    "clf_model = get_model(input_shape=input_shape, filter_blocks=filters)\n",
    "clf_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9215509",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model.compile(optimizer=\"adam\", loss=ssim_loss(), metrics=[\"MeanSquaredError\", \"Poisson\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23799157",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = clf_model.fit(x_2_train_final, x_2_train_final, \n",
    "                    epochs=28,\n",
    "                    batch_size=32,\n",
    "                    # callbacks = callbacks,\n",
    "                    validation_data=(x_2_test_final, x_2_test_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b93ca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = len(history.history.keys()) / 2\n",
    "metric = (key for key in history.history.keys())\n",
    "fig, ax = plt.subplots(2, 2, figsize=(8, 1.5*num))\n",
    "for j in range(2):\n",
    "  for i in range(int(num/2)):\n",
    "    this_metric = next(metric)\n",
    "    ax[i, j].plot(history.history[this_metric])\n",
    "    ax[i, j].plot(history.history[f\"val_{this_metric}\"])\n",
    "    ax[i, j].set_title(f'{this_metric}'.title())\n",
    "    ax[i, j].set(xlabel=\"Epochs\", ylabel=\"Loss\")\n",
    "plt.savefig(f\"{image_path}/ssim_output_graph_{run_count:03d}.png\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9279e2b",
   "metadata": {},
   "source": [
    "# Model 3 - WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d314d539",
   "metadata": {},
   "source": [
    "After spending a significant amount of time researching image detection methods and algorithms I\n",
    "decided to try to implement a simple version of the YOLO algorithm with my dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8df69cf",
   "metadata": {},
   "source": [
    "## Preparing Data and writing functions to calculate IoU and GIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad60129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = init_COCO(\"./data/\", [\"train\", \"val\", \"test\"])\n",
    "test = CategoricalDataGen('test', data, \"./data/images\", target_size=(384, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b180c4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of divisions the anchor boxes and loss function should use\n",
    "xdivs = 12\n",
    "ydivs = 9\n",
    "# Calling my dataset class and using it's get_labels function to dervive the correctly formatted data from the dataset\n",
    "labels = test.get_labels(divs=(ydivs, xdivs), num_classes=13, num_boxes=3,\n",
    "                input_size=(1440, 1920))\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127cb8be",
   "metadata": {},
   "source": [
    "### Generating \"predictions\" to check my calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c3cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an array with zeros\n",
    "# The shape is (batch_size, x_division, y_division, 13(classes) + 3(boxes) * 6(x, y, width, height, angle))\n",
    "preds = np.zeros((60, xdivs, ydivs, 13 + 3 * 6), dtype=np.float32)\n",
    "\n",
    "# add our label tensor to it; the model predicts three anchor boxes for each grid cell so we create three predictions\n",
    "preds[...,:19] = preds[...,:19] + labels\n",
    "preds[..., 19:25] = labels[..., 13:19] \n",
    "preds[..., 25:] = labels[..., 13:19] bb\n",
    "# b_e means \"box exists\" it is the probability score that a box is present in the cell.\n",
    "# we can also use this value (here is is always equal to the true value) to reduce rows we that do not contain values to zero\n",
    "b_e = preds[..., 13]\n",
    "b_e = b_e.reshape((preds.shape[:-1] + (1,)))\n",
    "print(b_e.shape)\n",
    "box_exists = np.concatenate((b_e, b_e, b_e, b_e, b_e), axis=-1)\n",
    "# establishing a known loss for the first anchor box prediction so that I can follow the calculations for a sanity check\n",
    "# if it becomes necessary\n",
    "known_loss = np.asarray([10, 10, 5, 20, np.pi * .05])\n",
    "known_loss = np.full(preds.shape[:-1] + (5,), known_loss, dtype=np.float32)\n",
    "\n",
    "# generating the loss values and adding them to the prediction tensor\n",
    "preds[..., 14:19] = known_loss + preds[..., 14:19]\n",
    "preds[..., 20:25] = box_exists * (preds[..., 20:25] + np.random.normal(0, 2, preds.shape[:-1] + (5,)))\n",
    "preds[..., 26:] = box_exists * (preds[..., 26:] + np.random.normal(0, 2, preds.shape[:-1] + (5,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3782ca0b",
   "metadata": {},
   "source": [
    "### Box Cutter\n",
    "I wrote a class to handle the many operations that need to happen.\n",
    "\n",
    "The purpose of these operations is to compare the <b>True Labels</b> with the <b>Predicted Labels</b>. All of these methods are chains of tensor operations. Please remember that the labels are structured as a center point (x,y), a width and height, and an angle of alignment (the angle the bound box is pointing in radians).\n",
    "\n",
    "So to calculate IoU and GIoU I have to:\n",
    "* Calculate the actual x, y values for each corner point\n",
    "* Calculate the intersections of the edges between the True bounding box and the Prediction\n",
    "* Check for corners floating in the middle of either bounding box\n",
    "* Return the intersections and 'inside' corners as a points of a polygon (this will be a different number of points for each pair in the two tensors)\n",
    "* Calculate the are of that n-sided polygon, this is the Intersection\n",
    "* Calculate IoU\n",
    "* Find the smallest rectangle that encompasses both boxes, and it's area\n",
    "* Calculate GIoU\n",
    "\n",
    "<b>Let's go!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efaae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a class I wrote to perform a complicated chain of tensor operations\n",
    "box_cutter = BoundingBox_Processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c4e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step was to extract the corner point x, y values from the labels\n",
    "label_corners = box_cutter.get_corners(labels)\n",
    "pred_corners = box_cutter.get_corners(preds)\n",
    "pred_corners = pred_corners[0]\n",
    "label_corners = label_corners[0]\n",
    "print(pred_corners.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e6ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then I wrote a class method that extracts the edges of each bounding box in the tensors\n",
    "true_edges = box_cutter.get_edges(label_corners)\n",
    "pred_edges = box_cutter.get_edges(pred_corners)\n",
    "print(pred_edges.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00c15a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_points = box_cutter.construct_intersection(label_corners, pred_corners, return_centered=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03983804",
   "metadata": {},
   "source": [
    "### Skipping to the End\n",
    "All of the cool code is in the box_cutter.py file in src/\n",
    "\n",
    "Below you can see the culmination of all that work. Nan values are cells where there is no bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bfdde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "iou = box_cutter.calculate_iou(label_corners, pred_corners)\n",
    "print(f\"IoU: {iou.shape}\\n{iou[0, 5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c57d44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "giou = box_cutter.calculate_GIoU(label_corners, pred_corners)\n",
    "giou[0, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854179f9",
   "metadata": {},
   "source": [
    "## Taking a Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a6e79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_box = label_corners[0, :, :]\n",
    "test_pred = pred_corners[0, :, :]\n",
    "test_intr = tf.constant(intersection_points[0, :, :, :8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05326f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.set(\n",
    "        xlim=[0, 512],\n",
    "        ylim=[0, 384],\n",
    "        xticks=list(range(0, 512,int(512/12))),\n",
    "        yticks=list(range(0, 384, int(384/9))),\n",
    "        )\n",
    "ax.grid(visible=True, zorder=0)\n",
    "ax.set_title(\"Example of Calculated Shape Intersection\")\n",
    "ax.imshow(np.asarray(load_img(\"./data/images/test/screws_006.png\", target_size=(384, 512))), zorder=0, alpha=.6)\n",
    "ax.set_facecolor('black')\n",
    "for cell in range(12 * 9):\n",
    "    cl = divmod(cell, 12) \n",
    "    check = tf.cast(tf.reduce_sum(test_box[cl[1], cl[0]]) < .001, dtype=tf.bool).numpy()\n",
    "    if check:\n",
    "        continue\n",
    "    mask = tf.cast(tf.reduce_sum(test_intr[cl[1], cl[0]], axis=-1) > 0.001, dtype=tf.bool)\n",
    "    test_inter_ma = tf.boolean_mask(test_intr[cl[1], cl[0]], mask)\n",
    "    ax.add_patch(Polygon(test_box[cl[1], cl[0]], fill=None, edgecolor='chartreuse', lw=1, zorder=20))\n",
    "    ax.add_patch(Polygon(test_pred[cl[1], cl[0]], fill=None, edgecolor='fuchsia', lw=1, zorder=20))\n",
    "    ax.add_patch(Polygon(test_inter_ma, facecolor='palegreen', edgecolor='springgreen', lw=1, alpha=.4, zorder=10))\n",
    "ax.axhline(0, -10, 10, lw=1, color='black', linestyle='--', zorder=5)\n",
    "ax.axvline(0, -10, 10, lw=1, color='black', linestyle='--', zorder=5)\n",
    "plt.savefig(\"./images/bbox_intersection.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ea8f90",
   "metadata": {},
   "source": [
    "## Animating the Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f878b2",
   "metadata": {},
   "source": [
    "#### This function just searches the tensor and returns polygon patches whenever it finds a valid bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b7b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cells(test_box, test_pred, test_intr):\n",
    "    result = []\n",
    "    print(test_box.shape)\n",
    "    x_dim, y_dim = test_box.shape[0:2]\n",
    "    for cell in range(x_dim * y_dim):\n",
    "        x, y = divmod(cell, x_dim) \n",
    "        check = tf.cast(tf.reduce_sum(test_box[y, x]) < .001, dtype=tf.bool).numpy()\n",
    "        if check:\n",
    "            continue\n",
    "        print(test_intr[y, x])\n",
    "        mask = tf.cast(tf.reduce_sum(test_intr[y, x], axis=-1) > 0.001, dtype=tf.bool)\n",
    "        y_intr = tf.boolean_mask(test_intr[y, x], mask)\n",
    "        print(y_intr)\n",
    "        y_true = test_box[y, x]\n",
    "        y_pred = test_pred[y, x]\n",
    "        result.append((y_true, y_pred, y_intr)) \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686901ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "poly_list = find_cells(test_box, test_pred, test_intr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8c4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "def animation(i):\n",
    "    if i > 0:\n",
    "        ax.clear()\n",
    "    ax.tick_params(axis=\"both\",\n",
    "                   which=\"both\",\n",
    "                   bottom=False,\n",
    "                   left=False,\n",
    "                   labelbottom=False,\n",
    "                   labelleft=False)\n",
    "    ax.set(\n",
    "            xlim=[0, 512],\n",
    "            ylim=[0, 384],\n",
    "            xticks=list(range(0, 512,int(512/12))),\n",
    "            yticks=list(range(0, 384, int(384/9))),\n",
    "            # yticklabels=np.linspace(0, 9, 10),\n",
    "            # xticklabels=np.linspace(0, 12, 13)\n",
    "            )\n",
    "    ax.grid(visible=True, color=\"black\", zorder=0)\n",
    "    ax.set_title(\"Example of Calculated Shape Intersection\")\n",
    "    ax.imshow(np.asarray(load_img(\"./data/images/test/screws_006.png\", target_size=(384, 512))), zorder=0, alpha=1)\n",
    "    ax.add_patch(Polygon(poly_list[i][0], fill=None, edgecolor='chartreuse', lw=1, zorder=20))\n",
    "    ax.add_patch(Polygon(poly_list[i][1], fill=None, edgecolor='fuchsia', lw=1, zorder=20))\n",
    "    ax.add_patch(Polygon(poly_list[i][2], facecolor='palegreen', edgecolor='springgreen', lw=1, alpha=.5, zorder=10))\n",
    "\n",
    "anim = FuncAnimation(fig, animation, frames=len(poly_list))\n",
    "# anim.save(\"./images/intr_anim3.gif\", fps=1)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bb8a12",
   "metadata": {},
   "source": [
    "# Model 4 - Final Model\n",
    "For my final object detection attempt I used a YOLO implmentation written with Darknet. Darknet is\n",
    "a Machine-Learning framework written in C and provides a python api and many versions of models with trained \n",
    "weights. I am using the YOLOv3 implementation in darknet and trained it using a set of weights from\n",
    "COCO and my own data. \n",
    "\n",
    "It was relatively easy to set up and get training. There were small adjustments I need to make to\n",
    "my dataset and simple scripts to translate my labels to a format darknet will accept. In order to\n",
    "use this framework I removed the angle parameter from the labels and recalculated the points to \n",
    "match what darknet expects.\n",
    "\n",
    "To run this section you would need to have my directory and project structure in place. This is currently only run on Google Colab, so I don't have a local version of the project.\n",
    "\n",
    "I will paste the code below though, with the steps needed to setup, train and validated <b>darknet</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c223b154",
   "metadata": {},
   "source": [
    "## darknet setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a42e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/AlexeyAB/darknet\n",
    "# You need to clone the repo into your working directory (/content/ on Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27f9107",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd darknet\n",
    "# These make edits to the C make file to tell it to compile for running on a GPU\n",
    "!sed -i 's/OPENCV=0/OPENCV=1/' Makefile\n",
    "!sed -i 's/GPU=0/GPU=1/' Makefile\n",
    "!sed -i 's/CUDNN=0/CUDNN=1/' Makefil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3c11a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile darknet\n",
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00417175",
   "metadata": {},
   "source": [
    "### My environment Setup\n",
    "* Connect Colab VM to my Google Drive\n",
    "* Copy tar balls containing my project files over to the VM\n",
    "* Unpack the folders and copy all the files into their correct positions\n",
    "* Copy the latest weights file over from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f634ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5320d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /content/gdrive/MyDrive/yoloV3/yolo_v3_0104.tar.gz /content/\n",
    "!cp /content/gdrive/MyDrive/yoloV3/images_yolo_0103.tar.gz /content/\n",
    "!cp /content/gdrive/MyDrive/yoloV3/yolov3_5l_last.weights /content/darknet/\n",
    "!tar -xf /content/yolo_v3_0104.tar.gz\n",
    "!tar -xf /content/images_yolo_0103.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /content/darknet/data/obj\n",
    "!cp /content/yolo_v3/obj/* /content/darknet/data/obj/\n",
    "!cp /content/yolo_v3/obj.* /content/darknet/data/\n",
    "!cp /content/images/train/* /content/darknet/data/obj/\n",
    "!cp /content/images/val/* /content/darknet/data/obj/\n",
    "!cp /content/images/test/* /content/darknet/data/obj/\n",
    "!cp /content/yolo_v3/yolov3_5l.cfg /content/darknet/cfg/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbe2bc9",
   "metadata": {},
   "source": [
    "## Train, or Test darknet\n",
    "To train you need to edit the cfg file and pass 'train' instead of 'test' as a commandline argument. My darknet model is still training with a loss of around .55. I will add a result from an earlier set of weights that leaves quite a bit to be desired.\n",
    "\n",
    "There is probably a mismatch between the way that darknet displays the coordinates and the way they are structured in the dataset, because the class predictions are not too bad, but the box predictions are not displayed correctly.\n",
    "\n",
    "Because I do not have direct access to the predictions of the model it is hard to check without retraining the model on a different label configuration. I will continue to tweak it, but until I can track down the source of this problem it is displaying the bounding boxes badly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d65f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./darknet detector test data/obj.data cfg/yolov3_5l.cfg yolov3_5l_last.weights /content/images/train/screws_004.png -thresh 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bb9a2d",
   "metadata": {},
   "source": [
    "![darknet output](./images/darknet_output.png)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,auto:percent",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
